# Autonomous Rsearch critic and synthesizer (Open-Source, Local LLMs, Powered by LangGraph)

This project is a multi-step **research orchestration engine** built using **LangGraph** and **open-source local models** (Ollama).  
It transforms a user query into structured subqueries, runs parallel research steps, evaluates the outputs for quality, checks for contradictions, and finally synthesizes a polished answer.

The system uses free, local models only â€” no OpenAI API or paid services required.

---

## ğŸš€ Features

### **Parallel Research**
- Each subquery is processed independently using:
  - `research_unit`
  - `critic_unit`
  - `factcheck_unit`
- LangGraph maps these nodes across all subqueries simultaneously.

### **Quality-Control Pipeline**
Each research output undergoes:
1. **Critic evaluation** (clarity and relevance)  
2. **Fact-check evaluation** (internal evidence weaknesses)  
3. **Contradiction check** (global consistency across outputs)

Weak outputs trigger automatic retries.

### **Modular Architecture**
Nodes are fully isolated and easy to extend:
- `research_unit`
- `critic_unit`
- `factcheck_unit`
- `contradiction`
- `synthesize`

Future nodes (planned Phase 3):
- Web search integration
- RAG-based fact validation
- Supervisor agent for subquery refinement
- Logging/visualization

---

## ğŸ§  How It Works (High-Level Flow)
User Query
â†“
Expand into subqueries
â†“
Parallel Research (map)
research_unit â†’ critic_unit â†’ factcheck_unit
â†“ (fan-in)
Postprocess results
â†“
Contradiction analysis
â†“
Synthesis of final answer
â†“
Output
---

## ğŸ›  Installation

### Install dependencies:

pip install -r requirements.txt


### Install Ollama (for local LLM inference):
https://ollama.com

Pull a model:

ollama pull llama3.1


---

## â–¶ï¸ Run the pipeline

python -m src.run --query "future of robotics in agriculture"


---

## ğŸ“‚ Project Structure

src/
â”œâ”€â”€ nodes/
â”‚ â”œâ”€â”€ research_unit.py
â”‚ â”œâ”€â”€ critic_unit.py
â”‚ â”œâ”€â”€ factcheck_unit.py
â”‚ â”œâ”€â”€ contradiction.py
â”‚ â”œâ”€â”€ synthesize.py
â”œâ”€â”€ state.py
â”œâ”€â”€ graph_builder.py
â””â”€â”€ run.py


---

## ğŸ§© About Models
All LLM calls use local free models via **Ollama**:


You can swap in any other Ollama model by editing the `model="..."` lines in the node files.

---

Export path using:-

# $env:PYTHONPATH = "$PWD;$env:PYTHONPATH"  ---> For Windows



## ğŸ“Œ Next Steps (Planned Features)

Phase 3 will add:
- Web search tool integration
- Local RAG (vector search)
- Evidence-aware fact-checking
- Supervisor node for intelligent subquery refinement
- Graph visualization via LangGraphâ€™s inspector

---

## ğŸ§‘â€ğŸ’» Author
Mayank Singh â€” Building intelligent research systems with LangGraph and open-source LLMs.
